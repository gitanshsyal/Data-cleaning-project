# Data-cleaning-project
Azure databricks data cleaning challenge-->

| Field         | Rule                                             | Source of validation |
| ------------- | ------------------------------------------------ | -------------------- |
| `CustomerID`  | Not null                                         | Inline               |
| `Name`        | Not null                                         | Inline               |
| `Email`       | Must not be empty and should contain â€œ@â€         | Inline               |
| `State`       | Must exist in `Valid_States` table in Azure SQL  | Lookup               |
| `CountryCode` | Must exist in `Valid_Country` table in Azure SQL | Lookup               |
| `Age`         | Not null and must be â‰¥ 18                        | Inline               |

ğŸ‘‰ If a record fails any rule, itâ€™s bad â†’ goes to /files/discarded/
ğŸ‘‰ If it passes all â†’ goes to /files/source/

As part of this mini project, I designed and implemented a data validation and cleaning pipeline that automatically separates good and bad records before storing them into the Azure Data Lake.

Tech Stack Used
==================================================================

Azure Data Lake Gen2 â€“ Storage

Azure SQL Database â€“ Lookup tables

Azure Key Vault â€“ Secret management

Azure Databricks (PySpark) â€“ Data processing

JDBC Connector â€“ For reading lookup data from Azure SQL

Steps I followed
==================================================================

Created required resources in Azure (Data Lake, SQL DB, Key Vault, Databricks).

Stored database credentials securely in Key Vault.

Loaded lookup tables (Valid_States, Valid_Country) from Azure SQL into Databricks using JDBC.

Read raw customer data from Data Lake.

Applied data quality checks using PySpark transformations.

Added a category column to classify records as target (good) or discarded (bad).

Wrote the processed data back to Data Lake partitioned by category.

